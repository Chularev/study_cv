{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RKgBwXr-YM8"
   },
   "source": [
    "# Задание 3.2 - сверточные нейронные сети (CNNs) в PyTorch\n",
    "\n",
    "Это упражнение мы буде выполнять в Google Colab - https://colab.research.google.com/  \n",
    "Google Colab позволяет запускать код в notebook в облаке Google, где можно воспользоваться бесплатным GPU!  \n",
    "\n",
    "Авторы курса благодарят компанию Google и надеятся, что праздник не закончится.\n",
    "\n",
    "Туториал по настройке Google Colab:  \n",
    "https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d  \n",
    "(Keras инсталлировать не нужно, наш notebook сам установит PyTorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcXBeP1O7cnY",
    "outputId": "fd0c07fe-1ea3-42ed-80b5-80e4f7b9d161"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.10.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.11.1)\n",
      "Requirement already satisfied: typing-extensions in /home/alex/.local/lib/python3.8/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/alex/.local/lib/python3.8/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.20.3)\n",
      "--2022-01-31 21:30:25--  http://ufldl.stanford.edu/housenumbers/train_32x32.mat\n",
      "Распознаётся ufldl.stanford.edu (ufldl.stanford.edu)... 171.64.68.10\n",
      "Подключение к ufldl.stanford.edu (ufldl.stanford.edu)|171.64.68.10|:80... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    Файл уже получен полностью; нет действий.\n",
      "\n",
      "--2022-01-31 21:30:28--  http://ufldl.stanford.edu/housenumbers/test_32x32.mat\n",
      "Повторное использование соединения с ufldl.stanford.edu:80.\n",
      "HTTP-запрос отправлен. Ожидание ответа... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    Файл уже получен полностью; нет действий.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intstall PyTorch and download data\n",
    "!pip3 install torch torchvision\n",
    "\n",
    "!wget -c http://ufldl.stanford.edu/housenumbers/train_32x32.mat http://ufldl.stanford.edu/housenumbers/test_32x32.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-afwWw-Q85vD"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision import transforms\n",
    "from pytorch_helper import PyTorchHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNU-OD9O9ltP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAt55B7I-YNA"
   },
   "source": [
    "# Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAvkoRx-9FsP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjNAClx3-YNB"
   },
   "source": [
    "Разделяем данные на training и validation.\n",
    "\n",
    "На всякий случай для подробностей - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRnr8CPg7Hli"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0bcioK6JBDK"
   },
   "source": [
    "# Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_lenet():\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(4),    \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*2*2, 10),\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_O9qiYySvuj"
   },
   "source": [
    "# Подбор гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyperparameters(config):\n",
    "    device = torch.device(\"cuda:0\") # Let's make sure GPU is available!\n",
    "    # First, lets load the dataset\n",
    "    data_train = dset.SVHN('./')\n",
    "    data_test = dset.SVHN('./', split='test')\n",
    "    # The key hyperparameters we're going to tune are learning speed, annealing rate and regularization\n",
    "    # We also encourage you to try different optimizers as well\n",
    "    batch_size = 64\n",
    "    helper = PyTorchHelper(batch_size,  data_train)\n",
    "\n",
    "    train_indices, val_indices = helper.split(.2)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "    val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n",
    "    #===============================================\n",
    "\n",
    "    Hyperparams = namedtuple(\"Hyperparams\", ['learning_rate', 'anneal_epochs', 'reg'])\n",
    "    RunResult = namedtuple(\"RunResult\", ['model', 'train_history', 'val_history', 'final_val_accuracy'])\n",
    "\n",
    "    learning_rates = [1e-1]\n",
    "    anneal_coeff = 0.2\n",
    "    anneal_epochs = [5]\n",
    "    regs = [0.0001]\n",
    "    optimizers = [optim.SGD]\n",
    "\n",
    "    batch_size = 64\n",
    "    epoch_num = config['epoch_num']\n",
    "\n",
    "    # Record all the runs here\n",
    "    # Key should be Hyperparams and values should be RunResult\n",
    "    run_record = {} \n",
    "\n",
    "    # Use grid search or random search and record all runs in run_record dictionnary \n",
    "    # Important: perform search in logarithmic space!\n",
    "\n",
    "    # TODO: Your code here!\n",
    "\n",
    "    best_hyperparams = Hyperparams(None, None, None)\n",
    "    best_result = RunResult(None, None, None, None)\n",
    "\n",
    "    for lr, reg, anneal_epoch, optimizer in product(learning_rates, regs, anneal_epochs, optimizers):\n",
    "        lenet_model =base_lenet() # base_lenet()\n",
    "\n",
    "        lenet_model.type(torch.cuda.FloatTensor)\n",
    "        lenet_model.to(device)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "\n",
    "        optimizer = optimizer(lenet_model.parameters(), lr=lr, weight_decay=reg)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=anneal_epoch, gamma=anneal_coeff)\n",
    "\n",
    "        params = Hyperparams(lr, anneal_epoch, reg)\n",
    "\n",
    "        print(f\"\\nCurrent hyperparams: {params}\")\n",
    "\n",
    "        loss_history, train_history, val_history = helper.train_model('best_lenet',lenet_model, train_loader, val_loader, loss, optimizer, epoch_num, scheduler)\n",
    "\n",
    "        result = RunResult(lenet_model, train_history, val_history, val_history[-1])\n",
    "        run_record[params] = result\n",
    "\n",
    "        if best_result.final_val_accuracy is None or best_result.final_val_accuracy < result.final_val_accuracy:\n",
    "            best_result = result\n",
    "            best_hyperparams = params\n",
    "\n",
    "        print(\"\\nCurrent best validation accuracy: %4.2f, best hyperparams: %s\" % (best_result.final_val_accuracy, best_hyperparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epoch_num': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current hyperparams: Hyperparams(learning_rate=0.1, anneal_epochs=5, reg=0.0001)\n",
      "Average loss: 1.376518, Train accuracy: 0.543528, Val accuracy: 0.777763\n",
      "\n",
      "Current best validation accuracy: 0.78, best hyperparams: Hyperparams(learning_rate=0.1, anneal_epochs=5, reg=0.0001)\n"
     ]
    }
   ],
   "source": [
    "find_hyperparameters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-31 21:38:25,418\tINFO services.py:1263 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-01-31 21:38:26,508\tWARNING function_runner.py:558 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.5/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/1.21 GiB heap, 0.0/0.6 GiB objects<br>Result logdir: /home/alex/ray_results/find_hyperparameters_2022-01-31_21-38-26<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>find_hyperparameters_732b7_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m 2022-01-31 21:38:28,585\tERROR function_runner.py:266 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 260, in run\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 328, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 594, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"<ipython-input-13-5b9fdcac4c66>\", line 4, in find_hyperparameters\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torchvision/datasets/svhn.py\", line 62, in __init__\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     raise RuntimeError('Dataset not found or corrupted.' +\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m RuntimeError: Dataset not found or corrupted. You can use download=True to download it\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m Exception in thread Thread-2:\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     self.run()\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 279, in run\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 260, in run\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 328, in entrypoint\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     return self._trainable_func(self.config, self._status_reporter,\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 594, in _trainable_func\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     output = fn()\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"<ipython-input-13-5b9fdcac4c66>\", line 4, in find_hyperparameters\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m   File \"/usr/local/lib/python3.8/dist-packages/torchvision/datasets/svhn.py\", line 62, in __init__\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m     raise RuntimeError('Dataset not found or corrupted.' +\n",
      "\u001b[2m\u001b[36m(pid=7995)\u001b[0m RuntimeError: Dataset not found or corrupted. You can use download=True to download it\n",
      "2022-01-31 21:38:28,788\tERROR trial_runner.py:773 -- Trial find_hyperparameters_732b7_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/trial_runner.py\", line 739, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/ray_trial_executor.py\", line 746, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\", line 82, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/worker.py\", line 1621, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TuneError): \u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=7995, ip=192.168.0.105, repr=<types.ImplicitFunc object at 0x7fc839e76100>)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/trainable.py\", line 178, in train_buffered\n",
      "    result = self.train()\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/trainable.py\", line 237, in train\n",
      "    result = self.step()\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 379, in step\n",
      "    self._report_thread_runner_error(block=True)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 526, in _report_thread_runner_error\n",
      "    raise TuneError(\n",
      "ray.tune.error.TuneError: Trial raised an exception. Traceback:\n",
      "\u001b[36mray::ImplicitFunc.train_buffered()\u001b[39m (pid=7995, ip=192.168.0.105, repr=<types.ImplicitFunc object at 0x7fc839e76100>)\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 260, in run\n",
      "    self._entrypoint()\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 328, in entrypoint\n",
      "    return self._trainable_func(self.config, self._status_reporter,\n",
      "  File \"/home/alex/.local/lib/python3.8/site-packages/ray/tune/function_runner.py\", line 594, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"<ipython-input-13-5b9fdcac4c66>\", line 4, in find_hyperparameters\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torchvision/datasets/svhn.py\", line 62, in __init__\n",
      "    raise RuntimeError('Dataset not found or corrupted.' +\n",
      "RuntimeError: Dataset not found or corrupted. You can use download=True to download it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for find_hyperparameters_732b7_00000:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.6/7.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/1.21 GiB heap, 0.0/0.6 GiB objects<br>Result logdir: /home/alex/ray_results/find_hyperparameters_2022-01-31_21-38-26<br>Number of trials: 1/1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>find_hyperparameters_732b7_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>find_hyperparameters_732b7_00000</td><td style=\"text-align: right;\">           1</td><td>/home/alex/ray_results/find_hyperparameters_2022-01-31_21-38-26/find_hyperparameters_732b7_00000_0_2022-01-31_21-38-27/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [find_hyperparameters_732b7_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ceced3d8809a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m analysis = tune.run(\n\u001b[0m\u001b[1;32m      2\u001b[0m        \u001b[0mfind_hyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m        \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m        resources_per_trial={\"cpu\": 8, \"gpu\": 1})\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [find_hyperparameters_732b7_00000])"
     ]
    }
   ],
   "source": [
    " analysis = tune.run(\n",
    "        find_hyperparameters,\n",
    "        num_samples=1,\n",
    "        resources_per_trial={\"cpu\": 8, \"gpu\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "Y6xExdw8JB1l",
    "outputId": "45e3e51e-25cd-426d-ce2b-e80e13c9e085"
   },
   "outputs": [],
   "source": [
    "best_val_accuracy = None\n",
    "best_hyperparams = None\n",
    "best_run = None\n",
    "\n",
    "for hyperparams, run_result in run_record.items():\n",
    "    if best_val_accuracy is None or best_val_accuracy < run_result.final_val_accuracy:\n",
    "        best_val_accuracy = run_result.final_val_accuracy\n",
    "        best_hyperparams = hyperparams\n",
    "        best_run = run_result\n",
    "        \n",
    "print(\"Best validation accuracy: %4.2f, best hyperparams: %s\" % (best_val_accuracy, best_hyperparams))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
