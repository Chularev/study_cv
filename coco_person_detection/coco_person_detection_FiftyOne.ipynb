{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Оглавление<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-loader\" data-toc-modified-id=\"Data-loader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data loader</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/mnt/heap/validation' if necessary\n",
      "Found annotations at '/mnt/heap/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'coco-2017' split 'validation'\n",
      " 100% |███████████████| 5000/5000 [22.9s elapsed, 0s remaining, 234.9 samples/s]      \n",
      "Dataset 'coco-2017-validation' created\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Download and load the validation split of COCO-2017\n",
    "fo_dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\",\n",
    "                               dataset_dir='/mnt/heap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone.utils.coco as fouc\n",
    "from viewer import Viewer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        fiftyone_dataset,\n",
    "        transforms=None,\n",
    "        gt_field=\"ground_truth\",\n",
    "        classes=None,\n",
    "    ):\n",
    "        self.samples = fiftyone_dataset\n",
    "        self.transforms = transforms\n",
    "        self.gt_field = gt_field\n",
    "\n",
    "        self.img_paths = self.samples.values(\"filepath\")\n",
    "\n",
    "        self.classes = classes\n",
    "        if not self.classes:\n",
    "            # Get list of distinct labels that exist in the view\n",
    "            self.classes = self.samples.distinct(\n",
    "                \"%s.detections.label\" % gt_field\n",
    "            )\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        sample = self.samples[img_path]\n",
    "        metadata = sample.metadata\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        detections = sample[self.gt_field].detections\n",
    "        img_has_person = 0\n",
    "        for det in detections:\n",
    "            if det.label != 'person':\n",
    "                continue\n",
    "            \n",
    "            category_id = self.labels_map_rev[det.label]\n",
    "            coco_obj = fouc.COCOObject.from_detection(\n",
    "                det, metadata, category_id=category_id,\n",
    "            )\n",
    "            x, y, w, h = coco_obj.bbox\n",
    "            \n",
    "            boxes.append([x/metadata['width'], \n",
    "                          y/metadata['height'], \n",
    "                          (x + w)/metadata['width'], \n",
    "                          (y + h)/metadata['height']])\n",
    "            \n",
    "            img_has_person = 1\n",
    "\n",
    "        target = {}\n",
    "        target['img_width'] = metadata['width']\n",
    "        target['img_height'] = metadata['height']\n",
    "       \n",
    "        target[\"box\"] = []\n",
    "        if img_has_person:\n",
    "            box = None\n",
    "            if len(boxes) > 1:\n",
    "                box = sorted(boxes)[0]\n",
    "            else:\n",
    "                box = boxes[0]\n",
    "            target[\"box\"] = torch.as_tensor(box, dtype=torch.float32)        \n",
    "            \n",
    "        target[\"img_has_person\"] = img_has_person\n",
    "        target[\"img_path\"] = img_path\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2693\n"
     ]
    }
   ],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "person_list = [\"person\"]\n",
    "person_view = fo_dataset.filter_labels(\"ground_truth\",\n",
    "        F(\"label\").is_in(person_list))\n",
    "\n",
    "print(len(person_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "img_size = (256, 256)\n",
    "train_transforms = T.Compose([T.Resize(img_size),\n",
    "                              T.ToTensor()])\n",
    "test_transforms = T.Compose([T.Resize(img_size),\n",
    "                             T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test set\n",
    "train_view = person_view.take(len(person_view) * 0.75, seed=51)\n",
    "test_view = person_view.exclude([s.id for s in train_view])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = FiftyOneTorchDataset(train_view, train_transforms,\n",
    "        classes=person_list)\n",
    "torch_dataset_test = FiftyOneTorchDataset(test_view, test_transforms, \n",
    "        classes=person_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "674"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "        torch_dataset, batch_size=100)\n",
    "    \n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "        torch_dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from pytorch_helper import PyTorchHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_xy = torch.nn.MSELoss()\n",
    "loss_function_bce = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_helper import PyTorchHelper\n",
    "\n",
    "def find_hyperparameters(config):\n",
    "    learning_rates = [1e-1]\n",
    "    anneal_coeff = 0.2\n",
    "    anneal_epochs = [5]\n",
    "    regs = config['regs']\n",
    "    optimizers = config['optimizers']\n",
    "\n",
    "    batch_size = 64\n",
    "    epoch_num = 1\n",
    "\n",
    "    run_record = {} \n",
    "    \n",
    "    helper = PyTorchHelper(8,  None)\n",
    "\n",
    "    lenet_model = None\n",
    "    val_loss = 8\n",
    "    loss_history = None\n",
    "    for lr, reg, anneal_epoch, optimizer in product(learning_rates, regs, anneal_epochs, optimizers):\n",
    "        lenet_model = config['model']() # base_lenet()\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        optimizer = optimizer(lenet_model.parameters(), lr=lr, weight_decay=reg)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=anneal_epoch, gamma=anneal_coeff)\n",
    "\n",
    "        lenet_model, loss_history, val_loss = helper.train_model(config['model_name'],lenet_model, data_loader, data_loader_test, loss, optimizer, epoch_num, scheduler)\n",
    "\n",
    "    return lenet_model, loss_history, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net():\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(in_features=512, out_features=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'regs': [0.0001],\n",
    "    'optimizers': [optim.SGD],\n",
    "    'model': net,\n",
    "    'model_name': 'best_lenet'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resource_monitor import ResourceMonitor\n",
    "resourceMonitor = ResourceMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_allocated: 0.0\n",
      "max_memory_allocated: 0.0\n",
      "memory_reserved: 0.0\n",
      "max_memory_reserved: 0.0\n"
     ]
    }
   ],
   "source": [
    "resourceMonitor.print_statistics('MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Start train:\n",
      "memory_allocated: 42.69677734375\n",
      "max_memory_allocated: 42.69677734375\n",
      "memory_reserved: 64.0\n",
      "max_memory_reserved: 64.0\n",
      "==============================\n",
      "Epoch 0/0\n",
      "----------\n",
      "Step 0/21 Loss 1.8146092891693115\n",
      "Step 1/21 Loss 7.3640007972717285\n",
      "Step 2/21 Loss 2.401801586151123\n",
      "Step 3/21 Loss 6.627871036529541\n",
      "Step 4/21 Loss 2.242035150527954\n",
      "Step 5/21 Loss 5.830135345458984\n",
      "Step 6/21 Loss 2.0152134895324707\n",
      "Step 7/21 Loss 4.953871250152588\n",
      "Step 8/21 Loss 1.946618914604187\n",
      "Step 9/21 Loss 3.9491846561431885\n",
      "Step 10/21 Loss 1.6774002313613892\n",
      "Step 11/21 Loss 3.1332740783691406\n",
      "Step 12/21 Loss 1.3065059185028076\n",
      "Step 13/21 Loss 1.8617403507232666\n",
      "Step 14/21 Loss 1.0305147171020508\n",
      "Step 15/21 Loss 1.5221433639526367\n",
      "Step 16/21 Loss 0.7700945734977722\n",
      "Step 17/21 Loss 1.0819052457809448\n",
      "Step 18/21 Loss 0.9018441438674927\n",
      "Step 19/21 Loss 0.6569605469703674\n",
      "Step 20/21 Loss 0.5315346121788025\n",
      "==============================\n",
      "Average loss train: 2.680963\n",
      "Average loss test: 0.550292\n",
      "==============================\n",
      "memory_allocated: 85.97119140625\n",
      "max_memory_allocated: 3662.26416015625\n",
      "memory_reserved: 3900.0\n",
      "max_memory_reserved: 3900.0\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "model, train_loss_history, val_loss_history = find_hyperparameters(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_allocated: 0.0\n",
      "max_memory_allocated: 3662.26416015625\n",
      "memory_reserved: 3900.0\n",
      "max_memory_reserved: 3900.0\n"
     ]
    }
   ],
   "source": [
    "resourceMonitor.print_statistics('MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-4873ce4e3145>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-4873ce4e3145>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ===============================================\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index):\n",
    "    img, target = torch_dataset[index]\n",
    "    pred = model(img.unsqueeze(0))[0]\n",
    "    \n",
    "    original_img = Image.open(target['i'])\n",
    "    viewer.print_prediction(img, target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) /tmp/pip-req-build-99ib2vsi/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 256\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a34a1ee83908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-063019811506>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Рабочий стол/python/yandex/study_cv/coco_person_detection/viewer.py\u001b[0m in \u001b[0;36mprint_prediction\u001b[0;34m(self, img, target, prediction)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Yes\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Рабочий стол/python/yandex/study_cv/coco_person_detection/viewer.py\u001b[0m in \u001b[0;36mprint_img\u001b[0;34m(self, img, target)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Yes\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_has_person'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'No'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_has_person'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Рабочий стол/python/yandex/study_cv/coco_person_detection/viewer.py\u001b[0m in \u001b[0;36madd_box\u001b[0;34m(self, img, target)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my_bottom_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'box'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_image_to_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         img = cv2.rectangle(img, (x_top_left, y_top_left), (x_bottom_right,y_bottom_right),\n\u001b[1;32m     32\u001b[0m                               (255, 0, 0), 4)\n",
      "\u001b[0;32m~/Рабочий стол/python/yandex/study_cv/coco_person_detection/viewer.py\u001b[0m in \u001b[0;36mconvert_from_image_to_cv2\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_from_image_to_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2BGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.4.0) /tmp/pip-req-build-99ib2vsi/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 256\n"
     ]
    }
   ],
   "source": [
    "predict(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "def convert_torch_predictions(preds, det_id, s_id, w, h, classes):\n",
    "    # Convert the outputs of the torch model into a FiftyOne Detections object\n",
    "    dets = []\n",
    "    #if preds[0] < 0.5:\n",
    "     #   detections = fo.Detections(detections=dets)\n",
    "      #  return detections, det_id\n",
    "    \n",
    "    # Parse prediction into FiftyOne Detection object\n",
    "    x0,y0,x1,y1 = preds[1:]\n",
    "    coco_obj = fouc.COCOObject(det_id, s_id, int(1), [x0, y0, x1-x0, y1-y0])\n",
    "    det = coco_obj.to_detection((w,h), classes)\n",
    "    dets.append(det)\n",
    "    det_id += 1\n",
    "        \n",
    "    detections = fo.Detections(detections=dets)\n",
    "        \n",
    "    return detections, det_id\n",
    "\n",
    "def add_detections(model, torch_dataset, view, field_name=\"predictions\"):\n",
    "    # Run inference on a dataset and add results to FiftyOne\n",
    "    torch.set_num_threads(1)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"Using device %s\" % device)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    image_paths = torch_dataset.img_paths\n",
    "    classes = torch_dataset.classes\n",
    "    det_id = 0\n",
    "    \n",
    "    with fo.ProgressBar() as pb:\n",
    "        for img, targets in pb(torch_dataset):\n",
    "            sample = view[targets[\"img_path\"]]\n",
    "            s_id = sample.id\n",
    "            w = sample.metadata[\"width\"]\n",
    "            h = sample.metadata[\"height\"]\n",
    "            \n",
    "            # Inference\n",
    "            preds = model(img.unsqueeze(0).to(device))[0]\n",
    "            \n",
    "            detections, det_id = convert_torch_predictions(\n",
    "                preds, \n",
    "                det_id, \n",
    "                s_id, \n",
    "                w, \n",
    "                h, \n",
    "                classes,\n",
    "            )\n",
    "            \n",
    "            sample[\"predictions\"] = detections\n",
    "            sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_detections(model, torch_dataset_test, fo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Оглавление",
   "title_sidebar": "Оглавление",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
