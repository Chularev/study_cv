{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Оглавление<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-loader\" data-toc-modified-id=\"Data-loader-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data loader</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handlers.data_preparer import get_datasets\n",
    "import torch\n",
    "from typing import Dict\n",
    "from viewer import Viewer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handlers.data_preparer import get_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to '/mnt/heap/validation' if necessary\n",
      "Found annotations at '/mnt/heap/raw/instances_val2017.json'\n",
      "Images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "person_view len = 1005\n",
      "train dataset = 753\n",
      "test dataset = 252\n"
     ]
    }
   ],
   "source": [
    "datasets = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(datasets) -> Dict[str,  torch.utils.data.DataLoader]:\n",
    "    return {\n",
    "        'train': torch.utils.data.DataLoader(\n",
    "            datasets['train'], batch_size=64, shuffle=True),\n",
    "        'val': torch.utils.data.DataLoader(\n",
    "            datasets['val'], batch_size=64, shuffle=False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = get_loaders(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7922, 0.8000, 0.8000,  ..., 0.7529, 0.7490, 0.7412],\n",
       "          [0.7686, 0.7882, 0.8039,  ..., 0.7412, 0.7490, 0.7412],\n",
       "          [0.7804, 0.7765, 0.7882,  ..., 0.7373, 0.7490, 0.7333],\n",
       "          ...,\n",
       "          [0.7725, 0.7569, 0.7608,  ..., 0.8196, 0.8275, 0.8353],\n",
       "          [0.7647, 0.7647, 0.7569,  ..., 0.8235, 0.8235, 0.8314],\n",
       "          [0.7529, 0.7725, 0.7686,  ..., 0.8353, 0.8353, 0.8353]],\n",
       " \n",
       "         [[0.7765, 0.7843, 0.7804,  ..., 0.7333, 0.7294, 0.7216],\n",
       "          [0.7529, 0.7686, 0.7804,  ..., 0.7255, 0.7373, 0.7255],\n",
       "          [0.7608, 0.7569, 0.7686,  ..., 0.7255, 0.7373, 0.7216],\n",
       "          ...,\n",
       "          [0.7569, 0.7569, 0.7608,  ..., 0.8118, 0.8196, 0.8275],\n",
       "          [0.7569, 0.7608, 0.7529,  ..., 0.8157, 0.8118, 0.8157],\n",
       "          [0.7490, 0.7686, 0.7647,  ..., 0.8275, 0.8235, 0.8196]],\n",
       " \n",
       "         [[0.7765, 0.7882, 0.7882,  ..., 0.7608, 0.7529, 0.7451],\n",
       "          [0.7647, 0.7804, 0.7882,  ..., 0.7529, 0.7608, 0.7529],\n",
       "          [0.7804, 0.7725, 0.7765,  ..., 0.7529, 0.7647, 0.7490],\n",
       "          ...,\n",
       "          [0.7490, 0.7490, 0.7529,  ..., 0.8235, 0.8314, 0.8314],\n",
       "          [0.7529, 0.7569, 0.7529,  ..., 0.8275, 0.8235, 0.8235],\n",
       "          [0.7412, 0.7608, 0.7647,  ..., 0.8275, 0.8314, 0.8235]]]),\n",
       " {'img_width': 640,\n",
       "  'img_height': 427,\n",
       "  'box': tensor([0.2292, 0.5595, 0.3871, 0.7325]),\n",
       "  'img_has_person': 1,\n",
       "  'img_path': '/mnt/heap/validation/data/000000074860.jpg'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger\n",
    "logger = Logger('TensorBoard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(i):\n",
    "    checkpoint_dir = '/mnt/heap/My folder/tune_reports/experiment_name/find_hyperparameters_32f52_00000_0_2022-02-22_16-24-13/checkpoint_0000'\n",
    "    s = '0' + str(i) if i < 10 else str(i)\n",
    "    checkpoint_dir += s\n",
    "    \n",
    "    torch_model = Net()\n",
    "    checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "    model_state, optimizer_state = torch.load(checkpoint)\n",
    "    torch_model.load_state_dict(model_state)\n",
    "    torch_model = torch_model.eval() \n",
    "    return torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(index, torch_model):\n",
    "    img, target = datasets['train'][index]\n",
    "    pred = torch_model(img.unsqueeze(0))\n",
    "    print('pred = {}, target = {}'.format(pred['bbox'], target['box']))\n",
    "    \n",
    "    \n",
    "    return viewer.get_img_with_predict(target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.2292, 0.5595, 0.3871, 0.7325])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.4524, 0.4771, 0.5553, 0.8022])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.0912, 0.1146, 0.7883, 0.7918])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.2617, 0.1258, 0.6953, 0.6697])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.0000, 0.5844, 0.2006, 0.9870])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.3778, 0.1798, 0.5098, 0.6989])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0., 0., 0., 0.])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.3691, 0.1213, 1.0000, 0.9881])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.3771, 0.5624, 0.4189, 0.7434])\n",
      "pred = tensor([[0., 0., 1., 1.]], grad_fn=<SliceBackward0>), target = tensor([0.1650, 0.2446, 0.2851, 0.6645])\n",
      "pred = tensor([[0.0024, 0.0952, 0.9477, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.2292, 0.5595, 0.3871, 0.7325])\n",
      "pred = tensor([[0.0021, 0.0912, 0.9512, 0.9995]], grad_fn=<SliceBackward0>), target = tensor([0.4524, 0.4771, 0.5553, 0.8022])\n",
      "pred = tensor([[0.0022, 0.0933, 0.9494, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.0912, 0.1146, 0.7883, 0.7918])\n",
      "pred = tensor([[0.0024, 0.0950, 0.9478, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.2617, 0.1258, 0.6953, 0.6697])\n",
      "pred = tensor([[0.0023, 0.0939, 0.9488, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.0000, 0.5844, 0.2006, 0.9870])\n",
      "pred = tensor([[0.0022, 0.0933, 0.9494, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.3778, 0.1798, 0.5098, 0.6989])\n",
      "pred = tensor([[0.0021, 0.0917, 0.9509, 0.9995]], grad_fn=<SliceBackward0>), target = tensor([0., 0., 0., 0.])\n",
      "pred = tensor([[0.0021, 0.0922, 0.9504, 0.9995]], grad_fn=<SliceBackward0>), target = tensor([0.3691, 0.1213, 1.0000, 0.9881])\n",
      "pred = tensor([[0.0022, 0.0933, 0.9494, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.3771, 0.5624, 0.4189, 0.7434])\n",
      "pred = tensor([[0.0022, 0.0931, 0.9496, 0.9994]], grad_fn=<SliceBackward0>), target = tensor([0.1650, 0.2446, 0.2851, 0.6645])\n",
      "pred = tensor([[0.0992, 0.3103, 0.7221, 0.9408]], grad_fn=<SliceBackward0>), target = tensor([0.2292, 0.5595, 0.3871, 0.7325])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/heap/My folder/python/study/study_cv/pytorch_framework/viewer.py:32: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  plt.figure()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred = tensor([[0.0703, 0.2801, 0.7540, 0.9617]], grad_fn=<SliceBackward0>), target = tensor([0.4524, 0.4771, 0.5553, 0.8022])\n",
      "pred = tensor([[0.0972, 0.3075, 0.7237, 0.9422]], grad_fn=<SliceBackward0>), target = tensor([0.0912, 0.1146, 0.7883, 0.7918])\n",
      "pred = tensor([[0.0978, 0.3091, 0.7236, 0.9419]], grad_fn=<SliceBackward0>), target = tensor([0.2617, 0.1258, 0.6953, 0.6697])\n",
      "pred = tensor([[0.0895, 0.3006, 0.7320, 0.9481]], grad_fn=<SliceBackward0>), target = tensor([0.0000, 0.5844, 0.2006, 0.9870])\n",
      "pred = tensor([[0.0859, 0.2966, 0.7357, 0.9507]], grad_fn=<SliceBackward0>), target = tensor([0.3778, 0.1798, 0.5098, 0.6989])\n",
      "pred = tensor([[0.0692, 0.2789, 0.7555, 0.9625]], grad_fn=<SliceBackward0>), target = tensor([0., 0., 0., 0.])\n",
      "pred = tensor([[0.0734, 0.2838, 0.7502, 0.9596]], grad_fn=<SliceBackward0>), target = tensor([0.3691, 0.1213, 1.0000, 0.9881])\n",
      "pred = tensor([[0.0890, 0.2998, 0.7325, 0.9484]], grad_fn=<SliceBackward0>), target = tensor([0.3771, 0.5624, 0.4189, 0.7434])\n",
      "pred = tensor([[0.0921, 0.3029, 0.7291, 0.9461]], grad_fn=<SliceBackward0>), target = tensor([0.1650, 0.2446, 0.2851, 0.6645])\n",
      "pred = tensor([[0.3418, 0.4281, 0.5614, 0.6932]], grad_fn=<SliceBackward0>), target = tensor([0.2292, 0.5595, 0.3871, 0.7325])\n",
      "pred = tensor([[0.3165, 0.4185, 0.5750, 0.7241]], grad_fn=<SliceBackward0>), target = tensor([0.4524, 0.4771, 0.5553, 0.8022])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(j)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     logger\u001b[38;5;241m.\u001b[39madd_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i), img)\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(index, torch_model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(index, torch_model):\n\u001b[0;32m----> 2\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch_model(img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, target = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m], target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m/mnt/heap/My folder/python/study/study_cv/pytorch_framework/data_handlers/dataset.py:34\u001b[0m, in \u001b[0;36mFiftyOneTorchDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     33\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_paths[idx]\n\u001b[0;32m---> 34\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[1;32m     36\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fiftyone/core/view.py:92\u001b[0m, in \u001b[0;36mDatasetView.__getitem__\u001b[0;34m(self, id_filepath_slice)\u001b[0m\n\u001b[1;32m     89\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: id_filepath_slice}\n\u001b[0;32m---> 92\u001b[0m view \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(view))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fiftyone/core/collections.py:3507\u001b[0m, in \u001b[0;36mSampleCollection.match\u001b[0;34m(self, filter)\u001b[0m\n\u001b[1;32m   3404\u001b[0m \u001b[38;5;129m@view_stage\u001b[39m\n\u001b[1;32m   3405\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mfilter\u001b[39m):\n\u001b[1;32m   3406\u001b[0m     \u001b[38;5;124;03m\"\"\"Filters the samples in the collection by the given filter.\u001b[39;00m\n\u001b[1;32m   3407\u001b[0m \n\u001b[1;32m   3408\u001b[0m \u001b[38;5;124;03m    Examples::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;124;03m        a :class:`fiftyone.core.view.DatasetView`\u001b[39;00m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_view_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fiftyone/core/view.py:803\u001b[0m, in \u001b[0;36mDatasetView._add_view_stage\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    801\u001b[0m     view \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mload_view(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     view \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m     view\u001b[38;5;241m.\u001b[39m_stages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m view\n",
      "File \u001b[0;32m/usr/lib/python3.8/copy.py:84\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     82\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__copy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/fiftyone/core/view.py:103\u001b[0m, in \u001b[0;36mDatasetView.__copy__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__copy__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dataset, _stages\u001b[38;5;241m=\u001b[39m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__stages\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/copy.py:137\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 137\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m y \u001b[38;5;241m=\u001b[39m memo\u001b[38;5;241m.\u001b[39mget(d, _nil)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _nil:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j in range(30):\n",
    "    model = get_model(j)\n",
    "    for i in range(10):\n",
    "        img = predict(i, model)\n",
    "        logger.add_image('img ' + str(i), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.3453e-07, 5.0985e-03, 9.9974e-01, 1.0000e+00]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Оглавление",
   "title_sidebar": "Оглавление",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
